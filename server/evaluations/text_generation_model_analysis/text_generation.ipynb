{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation model comparisons\n",
    "\n",
    "There are 3 candidate text generation models to utilise:\n",
    "- GPT-2 (Downloaded for local inference)\n",
    "- Gemma-2b-it (Downloaded for local inference)\n",
    "- Mixtral-8x7B-Instruct-v0.1 (API call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are an analyst from GXS Bank. Help me describe what you see in terms of trend with this JSON format: {\\\"Positive Insights\\\": <Paragraph>, \\\"Negative Insights\\\": <Paragraph>, \\\"Topic Insights\\\": <Paragraph>}. Do not put extra words like 'Based on...'. Output STRICTLY in JSON ONLY. Do not talk about null data.The following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_gpt2(prompt):\n",
    "    input_ids = gpt2_tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    output = gpt2_model.generate(input_ids, max_length=1024, num_return_sequences=1)\n",
    "\n",
    "    generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gpt2_output = generate_gpt2(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, logout\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770f0686a7e445038bd62bfad4d18c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91898ded1614373946b383d48959c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "def generate_gemma(prompt):\n",
    "    input_ids = gemma_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = gemma_model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "    generated_text = gemma_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_output = generate_gemma(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2ogpte import H2OGPTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"REDACTED\"\n",
    "client = H2OGPTE(address='https://h2ogpte.genai.h2o.ai', api_key=key)\n",
    "session_id = client.create_chat_session()\n",
    "\n",
    "def generate_mixtral(prompt):\n",
    "    with client.connect(session_id) as session:\n",
    "        response = session.query(prompt, timeout=70, rag_config={\"rag_type\": \"llm_only\", \"llm\": \"mistral-large-latest\", \"max_new_tokens\": 1024})\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixtral_output = generate_mixtral(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNTIME\n",
    "\n",
    "GPT-2: 30s\n",
    "Gemma-2b-it: 16mins 32s\n",
    "Mixtral-8x7B-Instruct-v0.1 (API Call): 14s\n",
    "\n",
    "Comparing runtime, using API for cloud inferencing is clearly the fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}'Jan 2024': 4,'Feb 2024': 3.6,'Mar 2024': 3.2,'Apr 2024': 2,'May 2024': 3.8,'Jun 2024': 4.5,'July 2024': 4.5,'Aug 2024': 4.2, 'Sep 2024': 3,'Oct 2024': 4,'Nov 2024': 3.7,'Dec 2024': 4.1}\\n\\nThe following is the overall data acquired from our banking application: {}\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Output:**\\n```json\\n{\\n  \"Positive Insights\": null,\\n  \"Negative Insights\": null,\\n  \"Topic Insights\": null\\n}\\n```'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"Positive Insights\": \"There is a positive trend in bank application usage from April 2024 to June 2024, with a peak in June at 4.5. Additionally, there is a consistent level of usage from July to December 2024, ranging from 3.7 to 4.5.\",\\n\"Negative Insights\": \"There is a decrease in bank application usage from March 2024 to May 2024, with a low of 3.2 in March and 3.8 in May.\",\\n\"Topic Insights\": \"Overall, the bank application usage shows a fluctuating trend throughout the year, with a general increase in usage from January to June 2024 and a slight decrease in the second half of the year.\"\\n}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixtral_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that mixtral has the most coherent output. GPT-2 simply repeats the prompt, and Gemma could not do proper analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
